{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import division\r\n",
    "from __future__ import print_function\r\n",
    "import os, sys\r\n",
    "import warnings\r\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\r\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\r\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\r\n",
    "\r\n",
    "import argparse\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import scipy.sparse as sp\r\n",
    "import torch\r\n",
    "\r\n",
    "SEED = 42\r\n",
    "np.random.seed(SEED)\r\n",
    "torch.manual_seed(SEED)\r\n",
    "torch.cuda.manual_seed(SEED)\r\n",
    "\r\n",
    "from torch import optim\r\n",
    "import torch.nn.functional as F\r\n",
    "from model import ROD_cluster\r\n",
    "from optimizer import loss_function\r\n",
    "from utils import *\r\n",
    "from sklearn.cluster import SpectralClustering, KMeans\r\n",
    "from clustering_metric import clustering_metrics\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "parser = argparse.ArgumentParser()\r\n",
    "parser.add_argument('--epochs', type=int, default=200)\r\n",
    "parser.add_argument('--num_hops', type=int, default=3)\r\n",
    "parser.add_argument('--dims', type=int, default=[64])\r\n",
    "parser.add_argument('--lr', type=float, default=1e-2)\r\n",
    "parser.add_argument('--batch_size', type=int, default=1000)\r\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4)\r\n",
    "parser.add_argument('--upd', type=int, default=10)\r\n",
    "parser.add_argument('--dataset', type=str, default='cora')\r\n",
    "parser.add_argument('--device', type=int, default=0)\r\n",
    "args = parser.parse_args(args=[])\r\n",
    "print(\"Using {} dataset\".format(args.dataset))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cora dataset\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.Data Fetching"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "if args.dataset == 'cora':\r\n",
    "    n_clusters = 7\r\n",
    "    lr = 1e-2\r\n",
    "    start_hops = 4\r\n",
    "elif args.dataset == 'citeseer':\r\n",
    "    n_clusters = 6\r\n",
    "    lr = 1e-3\r\n",
    "    start_hops = 3\r\n",
    "elif args.dataset == 'pubmed':\r\n",
    "    n_clusters = 3\r\n",
    "    lr = 1e-3\r\n",
    "    start_hops = 15\r\n",
    "\r\n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "adj, features, true_labels, idx_train, idx_val, idx_test = load_data(args.dataset)\r\n",
    "n_nodes, feat_dim = features.shape\r\n",
    "dims = [feat_dim] + args.dims\r\n",
    "\r\n",
    "adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\r\n",
    "adj.eliminate_zeros()\r\n",
    "\r\n",
    "n = adj.shape[0]\r\n",
    "\r\n",
    "adj_normalized = preprocess_graph(adj, norm='sym', renorm=True)\r\n",
    "features = sp.csr_matrix(features).toarray()\r\n",
    "\r\n",
    "for i in range(start_hops):\r\n",
    "    features = adj_normalized.dot(features)\r\n",
    "\r\n",
    "feature_list = [features]\r\n",
    "for i in range(args.num_hops):\r\n",
    "    feature_list.append(adj_normalized.dot(feature_list[-1]))\r\n",
    "input_feature = [torch.FloatTensor(feat).to(device) for feat in feature_list]\r\n",
    "\r\n",
    "adj_1st = (adj + sp.eye(n)).toarray()\r\n",
    "adj_label = torch.FloatTensor(adj_1st)\r\n",
    "neg_num = pos_num = adj_label.sum().long()\r\n",
    "\r\n",
    "model = ROD_cluster(dims, n_clusters, args.num_hops)\r\n",
    "\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sm_sim_mx_list = []\r\n",
    "for i in range(args.num_hops+1):\r\n",
    "    cur_feat = F.normalize(input_feature[i].cpu().data)\r\n",
    "    sm_sim_mx_list.append(torch.mm(cur_feat, cur_feat.t()).reshape([-1,]))\r\n",
    "\r\n",
    "adj_label = adj_label.reshape([-1,])\r\n",
    "model = model.to(device)\r\n",
    "\r\n",
    "pos_inds_list = []\r\n",
    "neg_inds_list = []\r\n",
    "for i in range(args.num_hops+1):\r\n",
    "    pos_inds_list.append(np.argpartition(-sm_sim_mx_list[i], pos_num)[:pos_num])\r\n",
    "    neg_inds_list.append(np.argpartition(sm_sim_mx_list[i], pos_num*200)[:pos_num*200])\r\n",
    "\r\n",
    "length = len(pos_inds_list[0])\r\n",
    "length_neg = len(neg_inds_list[0])\r\n",
    "\r\n",
    "pos_inds_cuda_list = [torch.LongTensor(pos_inds).to(device) for pos_inds in pos_inds_list]\r\n",
    "\r\n",
    "batch_size = args.batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "kmeans_list = [KMeans(n_clusters=n_clusters, n_init=20)]\r\n",
    "for _ in range(args.num_hops+1):\r\n",
    "    kmeans_list.append(KMeans(n_clusters=n_clusters, n_init=20))\r\n",
    "\r\n",
    "tqdm.write('Start Training...')\r\n",
    "for epoch in tqdm(range(args.epochs)):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    optimizer.zero_grad()\r\n",
    "    z_list = model(input_feature)\r\n",
    "    start, end = 0, batch_size\r\n",
    "    loss1 = 0.\r\n",
    "    ran_head = np.random.randint(0, length_neg-length-1)\r\n",
    "    sampled_neg_list = []\r\n",
    "    for i in range(args.num_hops+1):\r\n",
    "        sampled_neg_list.append(torch.LongTensor(neg_inds_list[i][np.arange(ran_head, ran_head+length)]).to(device))\r\n",
    "\r\n",
    "    if epoch % args.upd == 0:\r\n",
    "        label_list = []\r\n",
    "        centroid_list = []\r\n",
    "        for i in range(args.num_hops+1):\r\n",
    "            label_list.append(kmeans_list[i].fit_predict(z_list[i].data.cpu().numpy()))\r\n",
    "            centroid_list.append(kmeans_list[i].cluster_centers_)\r\n",
    "\r\n",
    "        new_label_list = [label_list[0]]\r\n",
    "        new_centroid_list = [torch.FloatTensor(centroid_list[0]).to(device)]\r\n",
    "\r\n",
    "        for i in range(1, args.num_hops+1):\r\n",
    "            temp_label, temp_index = munkres(label_list[i], label_list[0])\r\n",
    "            temp_centroid = np.array([centroid_list[i][temp_index[j][1]] for j in range(n_clusters)])\r\n",
    "            new_label_list.append(temp_label)\r\n",
    "            new_centroid_list.append(torch.FloatTensor(temp_centroid).to(device))\r\n",
    "\r\n",
    "    dist_list = []\r\n",
    "    for i in range(args.num_hops+1):\r\n",
    "        for j in range(n_clusters):\r\n",
    "            if j == 0:\r\n",
    "                dist = torch.norm(z_list[i] - new_centroid_list[i][j], p=2, dim=1, keepdim=True)\r\n",
    "            else:\r\n",
    "                dist = torch.cat((dist, torch.norm(z_list[i] - new_centroid_list[i][j], p=2, dim=1, keepdim=True)), 1)\r\n",
    "        dist_list.append(dist)\r\n",
    "\r\n",
    "    dist_norm_list = [F.softmax(dist, 1) for dist in dist_list]\r\n",
    "\r\n",
    "    attention_scores = [torch.sigmoid(model.lr_att2(dist_norm)).view(n_nodes, 1) for dist_norm in dist_norm_list]\r\n",
    "    W = torch.cat(attention_scores, dim=1)\r\n",
    "    W = F.softmax(W, 1)\r\n",
    "\r\n",
    "    dist_ensemble = torch.mul(dist_norm_list[0], W[:, 0].view(n_nodes, 1))\r\n",
    "    for i in range(1, args.num_hops+1):\r\n",
    "        dist_ensemble += torch.mul(dist_norm_list[i], W[:, i].view(n_nodes, 1))\r\n",
    "\r\n",
    "    label_ensemble = dist_ensemble.min(1)[1].long().cpu().numpy()\r\n",
    "    if len(list(set(label_ensemble))) < n_clusters:\r\n",
    "        y_pred = kmeans_list[args.num_hops+1].fit_predict(dist_ensemble.data.cpu().numpy())\r\n",
    "    else:\r\n",
    "        y_pred = label_ensemble\r\n",
    "\r\n",
    "    if epoch == 0:\r\n",
    "        cm = clustering_metrics(true_labels, y_pred)\r\n",
    "        best_acc, best_nmi, best_ari = cm.evaluationClusterModelFromLabel(tqdm)\r\n",
    "    else:\r\n",
    "        cm = clustering_metrics(true_labels, y_pred)\r\n",
    "        acc, nmi, ari = cm.evaluationClusterModelFromLabel(tqdm)\r\n",
    "        if acc > best_acc:\r\n",
    "            best_acc = acc\r\n",
    "            best_nmi = nmi\r\n",
    "            best_ari = ari\r\n",
    "\r\n",
    "    loss3 = 0.\r\n",
    "    for i in range(args.num_hops+1):\r\n",
    "        loss3 += F.mse_loss(dist_norm_list[i], dist_ensemble)\r\n",
    "\r\n",
    "    loss2 = 0.\r\n",
    "    for i in range(args.num_hops+1):\r\n",
    "        loss_tmp = -dist_list[i].mean(1).sum()\r\n",
    "        loss_tmp += 2 * np.sum([dist_list[i][j, x] for j, x in zip(range(dist_list[i].shape[0]), new_label_list[i])])\r\n",
    "        loss2 += loss_tmp / n_nodes\r\n",
    "\r\n",
    "    while end <= length:\r\n",
    "        for i in range(args.num_hops+1):\r\n",
    "            sampled_inds = torch.cat((pos_inds_cuda_list[i][start:end], sampled_neg_list[i][start:end]), 0)\r\n",
    "            xind = sampled_inds // n_nodes\r\n",
    "            yind = sampled_inds % n_nodes\r\n",
    "            zx = torch.index_select(z_list[i], 0, xind)\r\n",
    "            zy = torch.index_select(z_list[i], 0, yind)\r\n",
    "            batch_label = torch.cat((torch.ones(end-start), torch.zeros(end-start))).to(device)\r\n",
    "            batch_pred = (zx * zy).sum(1)\r\n",
    "            weight = torch.cat((batch_pred[:batch_size], 1-batch_pred[batch_size:]), 0).data\r\n",
    "            loss1 += loss_function(adj_preds=batch_pred, adj_labels=batch_label, weight=weight)\r\n",
    "\r\n",
    "        start = end\r\n",
    "        if end < length <= end + batch_size:\r\n",
    "            end += length - end\r\n",
    "        else:\r\n",
    "            end += batch_size\r\n",
    "\r\n",
    "    loss = 1*loss1 + 10*loss2 + 10*loss3\r\n",
    "    loss.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "\r\n",
    "tqdm.write(\"Optimization Finished!\")\r\n",
    "tqdm.write('best_acc: {}, best_nmi: {}, best_adj: {}'.format(best_acc, best_nmi, best_ari))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 200/200 [04:34<00:00,  1.37s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimization Finished!\n",
      "best_acc: 0.7470457902511078, best_nmi: 0.5810151004989821, best_adj: 0.5345240231222482\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}